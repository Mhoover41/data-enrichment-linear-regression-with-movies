{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2907bcdf",
   "metadata": {},
   "source": [
    "# Linear Regression with Statsmodels for Movie Revenue - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bcb42",
   "metadata": {},
   "source": [
    "- xx/xx/xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab6204b",
   "metadata": {},
   "source": [
    "## Activity: Create a Linear Regression Model with Statsmodels for Revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da70618",
   "metadata": {},
   "source": [
    "- Last Class:\n",
    "    - We started working with JUST the data data from the TMDB API for years 2000-2021. \n",
    "    - We prepared the data for modeling\n",
    "        - Some feature engineering\n",
    "        - Our usual Preprocessing\n",
    "        - New steps for statsmodels!\n",
    "    - We fit a statsmodels linear regression.\n",
    "    \n",
    "    \n",
    "- Today:\n",
    "    - We Will inspect the model summary.\n",
    "    - We will create the visualizations to check assumptions about the residuals.\n",
    "    - We will iterate upon our model until we meet the 4 assumptions as best we can.\n",
    "    - We will discuss tactics for dealing with violations of the assumptions. \n",
    "    - We will use our coefficients to make stakeholder recommendations (if theres time ðŸ¤ž)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7911e3c",
   "metadata": {},
   "source": [
    "> **[ðŸ•¹ Click here to jump to Part 2!](#ðŸ•¹-Part-2:-Checking-Model-Assumptions)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003ddd3",
   "metadata": {},
   "source": [
    "# ðŸ“º Previously, on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1b10d",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72280b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer, make_column_selector, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "## fixing random for lesson generation\n",
    "np.random.seed(321)\n",
    "\n",
    "##import statsmodels correctly\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d2b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1237",
   "metadata": {},
   "source": [
    "### ðŸ“š Finding & Loading Batches of Files with `glob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f396f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking what data we already in our Data folder using os.listdir\n",
    "import os\n",
    "FOLDER = 'Data/'\n",
    "file_list = sorted(os.listdir(FOLDER))\n",
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eb57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try loading in the first .csv.gz file from the list\n",
    "# pd.read_csv(file_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd4581",
   "metadata": {},
   "source": [
    "> Why isn't it working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6533213",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's check the filepath \n",
    "file_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the folder plus filename\n",
    "FOLDER+ file_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeec6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try read csv with folder plus filename\n",
    "pd.read_csv(FOLDER+ file_list[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed66cb0",
   "metadata": {},
   "source": [
    "- Now we would do that in a loop, and only want to open .csv.gz.\n",
    "- But there is a better way!\n",
    ">- Introducing `glob`\n",
    "    - Glob takes a filepath/query and will find every filename that matches the pattern provided.\n",
    "    - We use asterisks as wildcards in our query.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39266abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "## Make a filepath query\n",
    "q = FOLDER+\"*.csv.gz\"\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob.glob to get COMPLETE filepaths\n",
    "file_list = glob.glob(q)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620616f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob.glob to get COMPLETE filepaths\n",
    "q = FOLDER+\"final_*.csv.gz\"\n",
    "print(q)\n",
    "file_list = sorted(glob.glob(q))\n",
    "# file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc860ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(file_list[0],lineterminator ='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39251fe9",
   "metadata": {},
   "source": [
    "> But where are the rest of the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## in a sub-folder\n",
    "# os.listdir(FOLDER+'2010-2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303c35ef",
   "metadata": {},
   "source": [
    "- Recursive Searching with glob.\n",
    "    - add a `**/` in the middle of your query to grab any matches from all subfolders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d212eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use glob.glob to get COMPLETE filepaths\n",
    "q = FOLDER+\"/**/final_*.csv.gz\"\n",
    "print(q)\n",
    "file_list = sorted(glob.glob(q,recursive=True))\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list =[]\n",
    "for file in file_list:\n",
    "    temp_df = pd.read_csv(file,lineterminator='\\n')\n",
    "    df_list.append(temp_df)\n",
    "len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18171f",
   "metadata": {},
   "source": [
    "- Dealing with ParserErrors with \"possibly malformed files\"\n",
    "\n",
    "    - for a reason I do not fully understand yet, some of the files I downloaded error if I try to read them.\n",
    "        - \"ParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.`\n",
    "    - After some googling, the fix was to add `lineterminator='\\n'` to pd.read_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use a list comprehension to load in all files into 1 dataframe\n",
    "df = pd.concat([pd.read_csv(f,lineterminator='\\n') for f in file_list])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ids that are 0\n",
    "df = df.loc[ df['imdb_id']!='0']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ids that are 0  and then reset index\n",
    "df = df.loc[df['imdb_id']!='0']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4172fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the combined csv to disk\n",
    "df.to_csv(FOLDER+'combined_tmdb_data.csv.gz',compression='gzip',index=False)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264323fa",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd5fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns to exclude\n",
    "drop_cols = ['backdrop_path','backdrop_path','original_title','overview',\n",
    "                 'poster_path','status','tagline','id','homepage',\n",
    "                 'production_countries','video','production_companies','spoken_languages',\n",
    "            'original_language']\n",
    "df = df.drop(columns=drop_cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209567e8",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbac60",
   "metadata": {},
   "source": [
    "- Collection: convert to boolean\n",
    "- Genres: get just name and OHE\n",
    "- Cleaning Certification\n",
    "- Converting release date to year, month, and day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffa979",
   "metadata": {},
   "source": [
    "#### belongs to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f910f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 3,700+ movies that belong to collections\n",
    "df['belongs_to_collection'].notna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use .notna() to get True if it belongs to a collection\n",
    "df['belongs_to_collection'] = df['belongs_to_collection'].notna()\n",
    "df['belongs_to_collection'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9252467",
   "metadata": {},
   "source": [
    "#### genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ddbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[3,'genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get just the genre names as a list \n",
    "import json\n",
    "def get_genre_name(x):\n",
    "    x = x.replace(\"'\",'\"')\n",
    "    x = json.loads(x)\n",
    "    \n",
    "    genres = []\n",
    "    for genre in x:\n",
    "        genres.append(genre['name'])\n",
    "    return genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe3359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# use get_genre_name and convert all the genere name in list\n",
    "\n",
    "df['genre_list']= df['genres'].apply(get_genre_name)\n",
    "\n",
    "df_explode = df.explode('genre_list')\n",
    "df_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7849bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save unique genres\n",
    "#unique_genres = df_explode['genre_list'].unique()\n",
    "unique_genres = df_explode['genre_list'].dropna().unique()\n",
    "unique_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually One-Hot-Encode Genres\n",
    "for genre in unique_genres:\n",
    "    df[f\"Genre_{genre}\"] = df['genres'].str.contains(genre,regex =False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a68bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop original genre cols\n",
    "df  = df.drop(columns=['genres','genre_list'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656c4ccc",
   "metadata": {},
   "source": [
    "#### certification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24359a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking Certification values counts\n",
    "df['certification'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36348c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix extra space certs\n",
    "df['certification'] = df['certification'].str.strip()\n",
    "df['certification'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b898f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix certification col\n",
    "repl_cert = {'UR':'NR',\n",
    "             'Not Rated':'NR',\n",
    "             'Unrated':'NR',\n",
    "             '-':'NR',\n",
    "             '10':np.nan,\n",
    "             'ScreamFest Horror Film Festival':'NR'}\n",
    "df['certification'] = df['certification'].replace(repl_cert)\n",
    "df['certification'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35584817",
   "metadata": {},
   "source": [
    "#### Converting year to sep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9833f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split release date into 3 columns\n",
    "new_cols = ['year','month','day']\n",
    "df[new_cols] = df['release_date'].str.split('-',expand=True)\n",
    "df[new_cols] =df[new_cols].astype(float)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de084462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0127487",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop original feature\n",
    "df = df.drop(columns=['release_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226fe64f",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce70b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_for_model = ['title','imdb_id']\n",
    "df.drop(columns=drop_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fac20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make x and y variables\n",
    "\n",
    "y = df['revenue'].copy()\n",
    "X = df.drop(columns=['revenue',*drop_for_model]).copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)#, random_state=321)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce36522",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make cat selector and using it to save list of column names\n",
    "cat_select = make_column_selector(dtype_include='object')\n",
    "cat_cols = cat_select(X_train)\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d831fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make num selector and using it to save list of column names\n",
    "num_select = make_column_selector(dtype_include='number')\n",
    "num_cols = num_select(X_train)\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549690c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select manually OHE cols for later\n",
    "bool_select = make_column_selector(dtype_include='bool')\n",
    "already_ohe_cols = bool_select(X_train)\n",
    "already_ohe_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert manual ohe to int\n",
    "X_train[already_ohe_cols] = X_train[already_ohe_cols].astype(int)\n",
    "X_test[already_ohe_cols] = X_test[already_ohe_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07664258",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make pipelines\n",
    "cat_pipe = make_pipeline(SimpleImputer(strategy='constant',\n",
    "                                       fill_value='MISSING'),\n",
    "                         OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy='mean'),#StandardScaler()\n",
    "                        )\n",
    "\n",
    "preprocessor = make_column_transformer((num_pipe, num_cols),\n",
    "                                       (cat_pipe,cat_cols),remainder='passthrough')\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaf9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the col transformer\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Finding the categorical pipeline in our col transformer.\n",
    "preprocessor.named_transformers_['pipeline-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## B) Using list-slicing to find the encoder \n",
    "cat_features = preprocessor.named_transformers_['pipeline-2'][-1].get_feature_names_out(cat_cols)\n",
    "\n",
    "\n",
    "## Create the empty list\n",
    "final_features = [*num_cols,*cat_features,*already_ohe_cols]\n",
    "len(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473674db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = pd.DataFrame( preprocessor.transform(X_train), \n",
    "                          columns=final_features, index=X_train.index)\n",
    "X_train_tf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8157e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = pd.DataFrame( preprocessor.transform(X_test),\n",
    "                         columns=final_features, index=X_test.index)\n",
    "X_test_tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90364f8",
   "metadata": {},
   "source": [
    "### Adding a Constant for Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1970a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import statsmodels correctly\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67234ee8",
   "metadata": {},
   "source": [
    "> Tip: make sure that add_constant actually added a new column! You may need to change the parameter `has_constant` to \"add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make final X_train_df and X_test_df with constants added\n",
    "X_train_df = sm.add_constant(X_train_tf, prepend=False, has_constant='add')\n",
    "X_test_df = sm.add_constant(X_test_tf, prepend=False, has_constant='add')\n",
    "display(X_train_df.head(2),X_test_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210a57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2732fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0024ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30d54ef2",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6f9ed",
   "metadata": {},
   "source": [
    "### Statsmodels OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c4447",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## instantiate an OLS model WITH the training data.\n",
    "model = sm.OLS(y_train, X_train_df)\n",
    "\n",
    "## Fit the model and view the summary\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd82b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get train data performance from skearn to confirm matches OLS\n",
    "y_hat_train = result.predict(X_train_df)\n",
    "print(f'Training R^2: {r2_score(y_train, y_hat_train):.3f}')\n",
    "\n",
    "## Get test data performance\n",
    "y_hat_test = result.predict(X_test_df)\n",
    "print(f'Testing R^2: {r2_score(y_test, y_hat_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c13e6c",
   "metadata": {},
   "source": [
    "# The Assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64ec04",
   "metadata": {},
   "source": [
    "- The 4 Assumptions of a Linear Regression are:\n",
    "    - Linearity: That the input features have a linear relationship with the target.\n",
    "    - Independence of features (AKA Little-to-No Multicollinearity): That the features are not strongly related to other features.\n",
    "    - **Normality: The model's residuals are approximately normally distributed.**\n",
    "    - **Homoscedasticity: The model residuals have equal variance across all predictions.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509ac04",
   "metadata": {},
   "source": [
    "### QQ-Plot for Checking for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa2eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Q-QPlot\n",
    "\n",
    "# first calculate residuals \n",
    "resid = y_test - y_hat_test\n",
    "\n",
    "## then use sm's qqplot\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "sm.graphics.qqplot(resid,line='45',fit=True,ax=ax);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3ead1",
   "metadata": {},
   "source": [
    "### Residual Plot for Checking Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot scatterplot with y_hat_test vs resids\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.scatter(y_hat_test, resid, ec='white')\n",
    "ax.axhline(0,c='black',zorder=0)\n",
    "ax.set(ylabel='Residuals',xlabel='Predicted Revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338a052",
   "metadata": {},
   "source": [
    "### Putting it all together into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194aae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ols(result,X_train_df, y_train, show_summary=True):\n",
    "    \"\"\"Plots a Q-Q Plot and residual plot for a statsmodels OLS regression.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        display(result.summary())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ## save residuals from result\n",
    "    y_pred = result.predict(X_train_df)\n",
    "    resid = y_train - y_pred\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2,figsize=(12,5))\n",
    "    \n",
    "    ## Normality \n",
    "    sm.graphics.qqplot(resid,line='45',fit=True,ax=axes[0]);\n",
    "    \n",
    "    ## Homoscedasticity\n",
    "    ax = axes[1]\n",
    "    ax.scatter(y_pred, resid, edgecolor='white',lw=1)\n",
    "    ax.axhline(0,zorder=0)\n",
    "    ax.set(ylabel='Residuals',xlabel='Predicted Value');\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0f0f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_ols(result,X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c8842",
   "metadata": {},
   "source": [
    "# ðŸ•¹ Part 2: Checking Model Assumptions Continue...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d57fb",
   "metadata": {},
   "source": [
    "# Improving Our Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd39a4c",
   "metadata": {},
   "source": [
    "> \"Garbage In = Garbage Out\"\n",
    "\n",
    "\n",
    "- Before we dive into iterating on our model, I realized there were some big issues that I did not account for in the original data. \n",
    "    - some movies may not have been released. \n",
    "    - We should probably remove movies with 0 budget and revenue.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload the data\n",
    "df = pd.read_csv(FOLDER+'combined_tmdb_data.csv.gz',lineterminator='\\n')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7dddba",
   "metadata": {},
   "source": [
    "### Repeating Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24afcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns to exclude - Keeping Status and companies\n",
    "drop_cols = ['backdrop_path','backdrop_path','original_title','overview',\n",
    "                 'poster_path','tagline','id','homepage', #'status','production_companies'\n",
    "                 'production_countries','video','spoken_languages',\n",
    "            'original_language']\n",
    "df = df.drop(columns=drop_cols)\n",
    "\n",
    "## Use .notna() to get True if it belongs to a collection\n",
    "df['belongs_to_collection'] = df['belongs_to_collection'].notna()\n",
    "\n",
    "## Function to get just the genre names as a list \n",
    "import json\n",
    "def get_genre_name(x):\n",
    "    x = x.replace(\"'\",'\"')\n",
    "    x = json.loads(x)\n",
    "    \n",
    "    genres = []\n",
    "    for genre in x:\n",
    "        genres.append(genre['name'])\n",
    "    return genres\n",
    "\n",
    "## Use ourn function and exploding the new column\n",
    "df['genres_list'] = df['genres'].apply(get_genre_name)\n",
    "df_explode = df.explode('genres_list')\n",
    "\n",
    "## save unique genres\n",
    "unique_genres = df_explode['genres_list'].dropna().unique()\n",
    "\n",
    "## Manually One-Hot-Encode Genres\n",
    "for genre in unique_genres:\n",
    "    df[f\"Genre_{genre}\"] = df['genres'].str.contains(genre,regex=False)    \n",
    "\n",
    "\n",
    "## Drop original genre cols\n",
    "df = df.drop(columns=['genres','genres_list'])\n",
    "\n",
    "\n",
    "#### Fixing Certification\n",
    "## Checking Certification values\n",
    "df['certification'].value_counts(dropna=False)\n",
    "# fix extra space certs\n",
    "df['certification'] = df['certification'].str.strip()\n",
    "\n",
    "## fix certification col\n",
    "repl_cert = {'UR':'NR',\n",
    "             'Not Rated':'NR',\n",
    "             'Unrated':'NR',\n",
    "             '-':'NR',\n",
    "             '10':np.nan,\n",
    "             'ScreamFest Horror Film Festival':'NR'}\n",
    "df['certification'] = df['certification'].replace(repl_cert)\n",
    "df['certification'].value_counts(dropna=False)\n",
    "\n",
    "\n",
    "#### Converting year to sep features\n",
    "## split release date into 3 columns\n",
    "new_cols = ['year','month','day']\n",
    "df[new_cols] = df['release_date'].str.split('-',expand=True)\n",
    "df[new_cols] = df[new_cols].astype(float)\n",
    "\n",
    "## drop original feature\n",
    "df = df.drop(columns=['release_date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b68ea",
   "metadata": {},
   "source": [
    "### New Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cc26e",
   "metadata": {},
   "source": [
    "- Make sure to only keep:\n",
    "    1. Status=Released.\n",
    "    2. Budget >0\n",
    "    3. Revenue >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check status\n",
    "df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df872bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save only Released status\n",
    "df = df.loc[ df['status'] == 'Released']\n",
    "df = df.drop(columns=['status'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120591b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## filter out financials that don't have financial data\n",
    "df = df.loc[(df['budget'] >0 ) & (df['revenue']>0)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff4d70",
   "metadata": {},
   "source": [
    "### Production Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d38bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['production_companies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting longest string to check for multiple companies\n",
    "idxmax = df['production_companies'].apply(len).idxmax()\n",
    "idxmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[idxmax, 'production_companies']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expressions to extrap just the name\n",
    "import re\n",
    "exp= r\"\\'name\\'\\:.?\\'(\\w*.*?)\\'\"\n",
    "re.findall(exp, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b496980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_company_names(x):\n",
    "    if x=='[]':\n",
    "        return [\"MISSING\"]\n",
    "    \n",
    "    exp= r\"\\'name\\'\\:.?\\'(\\w*.*?)\\'\"\n",
    "    companies = re.findall(exp, x)\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a31ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## test function\n",
    "get_prod_company_names(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8709c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Save new clean prod_comapny col and explode\n",
    "df['prod_company'] = df['production_companies'].apply(get_prod_company_names)\n",
    "prod_companies = df['prod_company'].explode()\n",
    "prod_companies.value_counts().head(49)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ad329",
   "metadata": {},
   "source": [
    "- Common Prod Company Encoding:\n",
    "    - Keep top 50 most common companies an one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d0031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## saving the 50 most common companies\n",
    "common_companies = sorted(prod_companies.value_counts().head(50).index)\n",
    "common_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac384c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## manually ohe top 20 companies\n",
    "for company in common_companies:\n",
    "    df[f\"ProdComp_{company}\"] = df['production_companies'].str.contains(company, regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b5ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping columns\n",
    "drop_for_model = ['title','imdb_id','prod_company','production_companies']\n",
    "df = df.drop(columns=drop_for_model)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7c535",
   "metadata": {},
   "source": [
    "# Checking for Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfcac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## concatenating training data into plot_df\n",
    "plot_df = pd.concat([X_train_df,y_train],axis=1)\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save plot_cols list to show (dropping genre from plot_df from pair_plot)\n",
    "genre_cols = [c for c in df.columns if c.startswith('Genre')]\n",
    "plot_cols = plot_df.drop(columns=['revenue',*genre_cols]).columns\n",
    "len(plot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=plot_df, y_vars='revenue',x_vars=plot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8080b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot first 6 features\n",
    "sns.pairplot(data=plot_df, y_vars='revenue',x_vars=plot_cols[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=plot_df, y_vars='revenue',x_vars=plot_cols[6:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot remaining features\n",
    "\n",
    "sns.pairplot(data=plot_df, y_vars='revenue',x_vars=plot_cols[13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ca877",
   "metadata": {},
   "source": [
    "- Shouldn't have years before 2000, so drop. \n",
    "- Check outliers in popularity, runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35871d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove movies prior to 2000\n",
    "df = df.loc[ df['year']>=2000]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d752c",
   "metadata": {},
   "source": [
    "> Now need to recreate X and y varaibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb5f540",
   "metadata": {},
   "source": [
    "### Functionize ALL of the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fdff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(df_, y_col='revenue',drop_cols=[]):\n",
    "    \n",
    "    ## Make copy of input df\n",
    "    df = df_.copy()\n",
    "    \n",
    "    ## filter columns in drop cols (if exist)\n",
    "    final_drop_cols = []\n",
    "    [df.drop(columns=c,inplace=True) for c in df.columns if c in drop_cols]\n",
    "    \n",
    "    \n",
    "    ## Make x and y variables\n",
    "    y = df[y_col].copy()\n",
    "    X = df.drop(columns=[y_col]).copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y)#, random_state=321)\n",
    "    \n",
    "\n",
    "    \n",
    "    ## make cat selector and using it to save list of column names\n",
    "    cat_select = make_column_selector(dtype_include='object')\n",
    "    cat_cols = cat_select(X_train)\n",
    "\n",
    "\n",
    "    ## make num selector and using it to save list of column names\n",
    "    num_select = make_column_selector(dtype_include='number')\n",
    "    num_cols = num_select(X_train)\n",
    "\n",
    "\n",
    "    ## select manually OHE cols for later\n",
    "    bool_select = make_column_selector(dtype_include='bool')\n",
    "    already_ohe_cols = bool_select(X_train)\n",
    "\n",
    "    ## convert manual ohe to int\n",
    "    X_train[already_ohe_cols] = X_train[already_ohe_cols].astype(int)\n",
    "    X_test[already_ohe_cols] = X_test[already_ohe_cols].astype(int)\n",
    "\n",
    "    ## make pipelines\n",
    "    cat_pipe = make_pipeline(SimpleImputer(strategy='constant',\n",
    "                                           fill_value='MISSING'),\n",
    "                             OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    num_pipe = make_pipeline(SimpleImputer(strategy='mean'),#StandardScaler()\n",
    "                            )\n",
    "\n",
    "    preprocessor = make_column_transformer((num_pipe, num_cols),\n",
    "                                           (cat_pipe,cat_cols),remainder='passthrough')\n",
    "    \n",
    "    \n",
    "\n",
    "    ## fit the col transformer\n",
    "    preprocessor.fit(X_train)\n",
    "\n",
    "    ## Finding the categorical pipeline in our col transformer.\n",
    "    preprocessor.named_transformers_['pipeline-2']\n",
    "\n",
    "    ## B) Using list-slicing to find the encoder \n",
    "    cat_features = preprocessor.named_transformers_['pipeline-2'][-1].get_feature_names_out(cat_cols)\n",
    "\n",
    "\n",
    "    ## Create the empty list\n",
    "    final_features = [*cat_features,*num_cols,*already_ohe_cols]\n",
    "\n",
    "    ## Make df verisons of x data\n",
    "    X_train_tf = pd.DataFrame( preprocessor.transform(X_train), \n",
    "                              columns=final_features, index=X_train.index)\n",
    "\n",
    "\n",
    "    X_test_tf = pd.DataFrame( preprocessor.transform(X_test),\n",
    "                             columns=final_features, index=X_test.index)\n",
    "\n",
    "\n",
    "    ### Adding a Constant for Statsmodels\n",
    "    ## Make final X_train_df and X_test_df with constants added\n",
    "    X_train_df = sm.add_constant(X_train_tf, prepend=False, has_constant='add')\n",
    "    X_test_df = sm.add_constant(X_test_tf, prepend=False, has_constant='add')\n",
    "    return X_train_df, y_train, X_test_df, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use our function to make new x,y vars\n",
    "X_train_df, y_train, X_test_df, y_test = get_train_test_split(df)\n",
    "\n",
    "## instantiate an OLS model WITH the training data.\n",
    "model = sm.OLS(y_train, X_train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86005b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Fit the model and view the summary\n",
    "result = model.fit()\n",
    "evaluate_ols(result,X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3edf08",
   "metadata": {},
   "source": [
    "> How did we do? Did we meet the assumptions better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1724ed",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baeeaac",
   "metadata": {},
   "source": [
    "### Using Z-Score Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ba8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "def find_outliers(data, verbose=True):\n",
    "    outliers = np.abs(stats.zscore(data))>3\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"- {outliers.sum()} outliers found in {data.name} using Z-Scores.\")\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb8b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save a dictionary of the T/F outlier index for each feature in outleir_cols\n",
    "outlier_cols = ['runtime','popularity','revenue']\n",
    "\n",
    "outliers = {}\n",
    "for col in outlier_cols:\n",
    "    outliers_col = find_outliers(df[col])\n",
    "    outliers[col] = outliers_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new df_clean copy of df\n",
    "df_clean = df.copy()\n",
    "\n",
    "## loop through dictionary to remove outliers\n",
    "for col, idx_outliers in outliers.items():\n",
    "    df_clean = df_clean[~idx_outliers]\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4012a1",
   "metadata": {},
   "source": [
    "### Model 3(?): Outliers Removed (Z_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891d64b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_df, y_train, X_test_df, y_test = get_train_test_split(df_clean)\n",
    "## instantiate an OLS model WITH the training data.\n",
    "model = sm.OLS(y_train, X_train_df)\n",
    "\n",
    "## Fit the model and view the summary\n",
    "result = model.fit()\n",
    "evaluate_ols(result,X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd222a",
   "metadata": {},
   "source": [
    "### Removing Outliers - Using IQR Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d69b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualzie outlier-removed target\n",
    "sns.boxplot(x = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc403fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_IQR(data, verbose=True):\n",
    "    q3 = np.quantile(data,.75)\n",
    "    q1 = np.quantile(data,.25)\n",
    "\n",
    "    IQR = q3 - q1\n",
    "    upper_threshold = q3 + 1.5*IQR\n",
    "    lower_threshold = q1 - 1.5*IQR\n",
    "    \n",
    "    outliers = (data<lower_threshold) | (data>upper_threshold)\n",
    "    if verbose:\n",
    "        print(f\"- {outliers.sum()} outliers found in {data.name} using IQR.\")\n",
    "        \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec9b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_z = find_outliers(df['revenue'])\n",
    "outliers_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_iqr = find_outliers_IQR(df['revenue'])\n",
    "outliers_iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop to remove outliers from same clumns using new function\n",
    "outlier_cols = ['runtime','popularity','revenue']\n",
    "\n",
    "## Empty dict for both types of outliers\n",
    "outliers_z = {}\n",
    "outliers_iqr = {}\n",
    "\n",
    "## Use both functions to see the comparison for # of outliers\n",
    "for col in outlier_cols:\n",
    "    outliers_col_z = find_outliers(df[col])\n",
    "    outliers_z[col] = outliers_col_z\n",
    "    \n",
    "    outliers_col_iqr = find_outliers_IQR(df[col])\n",
    "    outliers_iqr[col] = outliers_col_iqr\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_outliers \n",
    "df_clean_z = df.copy()\n",
    "for col, idx_outliers in outliers_z.items():\n",
    "    df_clean_z = df_clean_z[~idx_outliers]\n",
    "df_clean_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b35d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_outliers  to create df_clean_iqr\n",
    "# remove_outliers\n",
    "df_clean_iqr = df.copy()\n",
    "for col, idx_outliers in outliers_iqr.items():\n",
    "    df_clean_iqr = df_clean_iqr[~idx_outliers]\n",
    "df_clean_iqr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a53c5f",
   "metadata": {},
   "source": [
    "### Model 4 - IQR Outliers Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04becbce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## MAKE NEW MODEL WITH IQR OUTLIERS REMOVED\n",
    "\n",
    "X_train_df, y_train, X_test_df, y_test = get_train_test_split(df_clean_iqr)\n",
    "## instantiate an OLS model WITH the training data.\n",
    "model = sm.OLS(y_train, X_train_df)\n",
    "\n",
    "## Fit the model and view the summary\n",
    "result = model.fit()\n",
    "display(result.summary())\n",
    "evaluate_ols(result,X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31cd331",
   "metadata": {},
   "source": [
    "> How are we doing??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d79ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get pvalues from model result\n",
    "pvals = result.pvalues \n",
    "pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2848a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for p-values that are >.05\n",
    "pvals[ pvals>.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5d7819",
   "metadata": {},
   "source": [
    "## Removing features - based on p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get list of ALL genre columns to see how many are sig\n",
    "genre_cols = [c for c in df_clean.columns if c.startswith(\"Genre\")]\n",
    "genre_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save just genre pvalues\n",
    "genre_pvals = pvals[genre_cols]\n",
    "## calc what % are insig?\n",
    "genre_pvals.sum()/len(genre_pvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca985086",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get list of ALL prod_comp columns to see how many are sig\n",
    "comp_cols = [c for c in df_clean.columns if c.startswith(\"ProdComp\")]\n",
    "comp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b273ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save just genre pvalues\n",
    "comp_pvals = pvals[comp_cols]\n",
    "comp_pvals.sum()/len(comp_pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e5a18",
   "metadata": {},
   "source": [
    "> both have <50% bad pvalues. Keep both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## what pvals are remaining?\n",
    "other_pvals = pvals.drop([*comp_cols, *genre_cols])\n",
    "other_pvals[other_pvals>.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be970de3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Make x,y vars, but drop features with bad pvalues\n",
    "df_clean_iqr = df_clean_iqr.drop(columns=['adult','budget'])\n",
    "X_train_df, y_train, X_test_df, y_test = \\\n",
    "                                get_train_test_split(df_clean_iqr,)\n",
    "## MAKE AND EVALUATE OLS\n",
    "## instantiate an OLS model WITH the training data.\n",
    "model = sm.OLS(y_train, X_train_df)\n",
    "\n",
    "## Fit the model and view the summary\n",
    "result = model.fit()\n",
    "evaluate_ols(result,X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb48ba",
   "metadata": {},
   "source": [
    "# Addressing Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the mask to hide the upper-right of the triangle\n",
    "corr = X_train_df.corr().abs()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr))\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(corr,square=True, cmap='Reds', annot=True, mask=mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5857c",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    " \n",
    "# separate just x-data and subtract mean\n",
    "features = X_train_df -  X_train_df.mean()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a list of VIF scores for each feature in features.\n",
    "vif_scores = [variance_inflation_factor(features.values, i) for i in range(len(features.columns))]\n",
    "\n",
    "# create a new dataframe to hold the VIF scores \n",
    "VIF = pd.Series(vif_scores, index=features.columns)\n",
    "VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sort VIF for inspect\n",
    "VIF.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623da5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set float format to view vals not in scientfic notation\n",
    "pd.set_option('display.float_format',lambda x: f'{x:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove infinite values from VIF and sort\n",
    "VIF = VIF[VIF!=np.inf].sort_values()\n",
    "VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter for VIF that are > 5\n",
    "VIF[VIF>5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58a8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save name of features with high vif\n",
    "high_vif = VIF[VIF>5].index\n",
    "high_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16692810",
   "metadata": {},
   "source": [
    "### FInal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800940f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## make new df_final copy of prev df\n",
    "df_final = df_clean_iqr.drop(columns =high_vif).copy()\n",
    "X_train_df, y_train, X_test_df, y_test = get_train_test_split(df_final)\n",
    "## instantiate an OLS model WITH the training data.\n",
    "model = sm.OLS(y_train, X_train_df)\n",
    "\n",
    "## Fit the model and view the summary\n",
    "result = model.fit()\n",
    "evaluate_ols(result,X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get train data performance from skearn to confirm matches OLS\n",
    "y_hat_train = result.predict(X_train_df)\n",
    "print(f'Training R^2: {r2_score(y_train, y_hat_train):.3f}')\n",
    "\n",
    "## Get test data performance\n",
    "y_hat_test = result.predict(X_test_df)\n",
    "print(f'Testing R^2: {r2_score(y_test, y_hat_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Coefficients\n",
    "plt.figure(figsize=(5,16))\n",
    "ax =result.params.sort_values().plot(kind='barh')\n",
    "ax.axvline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ba600",
   "metadata": {},
   "source": [
    "## Compare to Alternative Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bcd90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X_train_df, y_train, X_test_df, y_test = get_train_test_split(df)\n",
    "\n",
    "reg = RandomForestRegressor(verbose=1,random_state=42)\n",
    "reg.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633edb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get train data performance from skearn to confirm matches OLS\n",
    "y_hat_train = reg.predict(X_train_df)\n",
    "print(f'Training R^2: {r2_score(y_train, y_hat_train):.3f}')\n",
    "\n",
    "## Get test data performance\n",
    "y_hat_test = reg.predict(X_test_df)\n",
    "print(f'Testing R^2: {r2_score(y_test, y_hat_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_ols(reg, X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cbd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(reg.feature_importances_, index=X_train_df.columns)\n",
    "importances.sort_values().tail(25).plot(kind='barh',figsize=(6,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087413dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dojo-env)",
   "language": "python",
   "name": "dojo-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
